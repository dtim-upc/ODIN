# Intent Specification to Workflow Generation


## Requirements

- Python (3.11)
    - List of packages in `requirements.txt` (install with `pip install -r requirements.txt`)
        - PySHACL package available at [DTIM PySHACL repo](https://github.com/dtim-upc/pySHACL) (`requirements.txt`
          already
          has the correct link).
- Node.js (for the demo)
    - List of packages in `package.json` (install with `npm install`)

## Directory Structure

- `api/`: Code to initialize the API and respond to the requests. Check usage in [below](#demo)
- `common/`: Common code for ontology and workflow generation, essentially namespace definition and base graph
  generation.
- `dataset_annotator/`: Code for annotating datasets with ontology terms.
- `demo_scripts/`: Scripts to showcase the interaction with the API. Check usage in [section below](#demo).
- `ontologies/`: Ontology used in the project. Divided in three files:
    - [`tbox.ttl`](./ontologies/tbox.ttl): Schema of the Ontology.
    - [`cbox.ttl`](./ontologies/cbox.ttl): Taxonomies of the Ontology.
    - [`abox.ttl`](./ontologies/abox.ttl): Instances of the Ontology.
- `ontology_populator/`: Code for generating the ontology.
- `pipeline_generator/`: Code for generating workflows.
- `pipeline_tranlsator`: Code for translating ontology workflows into KNIME workflows.

## API

Before executing any code, make sure you have all the dependencies installed (see [Requirements](#requirements)). Additionally,
we first have to generate the ontology that will be employed to create the intents. To do so, check the 
[ontology generator](#ontology-generator) section.

The easiest way to interact with the system and explore everything it has to offer is via the API defined in the 
[`api`](./api) directory. To run it, execute the following command from the project root directory:

```shell
flask --app ./api/api_main.py run --port 5001
```
The script [`demo_api_interaction`](./demo_scripts/demo_api_interaction.py) in the [`demo_script`](./demo_scripts) folder showcases 
the main interactions with the API in order to generate an intent. The main steps are as follows:

1. Obtain the list of available problems (classification, regression, etc.). This list of tasks is defined in the 
ontology, and displays what processes can be employed. **As of now, only classification tasks are implemented**
2. Annotate dataset: a dataset needs to be annotated (i.e. digested as an RDF file with the required properties
specified) before serving as input for the intent-generation process.
3. Create abstract plans. These plans are general structures to solve a task. One such plans is generated per algorithm
that implements the selected problem. **As of now, only three algorithms are defined for the Classification task: 
Decision Trees, SVMs and NN** (the first two fully implemented with KNIME conversion, the latter only defined).
4. Create logical plans. These plans define in a finer-grain manner how to execute each workflow, taking into account
the requirements of the selected algorithm. Moreover, several logical plans are generated for every abstract plan, 
employing different methodologies to implement each task. For instance, SVMs can have different types of kernel, and they 
require normalization and removal of non-null values. Moreover, it is necessary to perform a training-test split.
Currently, the ontology defines 3 types of kernel, 3 normalization alternatives, 2 methodologies to remove null values
and 4 splitting strategies. In total, we have 3 * 3 * 2 * 4 = 72 logical plans.
5. (Optional) Download the resulting RDF file.
6. (Optional) Translate the workflows (RDF format) into KNIME workflows.

Notice that `demo_api_interaction` includes a series of predefined parameters for the generation of the workflows, which
ensures a successful execution. The currently available alternatives are also displayed.

## Ontology Populator

Scripts and classes to generate the TBox and the CBox.

### TBox

The TBox can be generated by the [`tbox_generator.py`](./ontology_populator/tbox_generator.py) script, which stores it
in the [`ontologies`](./ontologies) directory (you can pass an alternative destination path as a parameter to the
script).

  ```bash
  cd ontology_populator
  python3 tbox_generator.py
  ```

### CBox

The CBox can be generated by the [`cbox_generator.py`](./ontology_populator/cbox_generator.py) script, which stores it
in the [`ontologies`](./ontologies) directory (you can pass an alternative destination path as a parameter to the
script).

  ```bash
  cd ontology_populator
  python3 cbox_generator.py
  ```

It adds the following elements:

- Problems: Specified in the script along their hierarchy
- Algorithms: Specified in the script along the problem they solve
- Models: Specified in the script
- Shapes: Defined in the script
- Implementations: Defined in the implementations directory
- Components: Defined in the implementations directory
- Parameters: Defined in the implementations directory
- Transformations: Defined in the implementations directory

#### Implementations and related entities

Implementations and their related entities (namely components, parameters and transformations) are defined in
the [`implementations`](./ontology_populator/implementations) directory. The directory contains
the [`core` package](./ontology_populator/implementations/core), defining the base classes, and the
[`knime` package](./ontology_populator/implementations/knime), which contains subclasses of the base ones adding
KNIME-specific behaviour, and specifies all the implementations (and related entities) available.

The classes in the `core` package are:

- [`Implementation`](./ontology_populator/implementations/core/implementation.py): Base class for implementations.
  Contains all the information related to the implementation (name, algorithm, parameters, inputs, and outputs), and is
  responsible for creating the RDF triples for the implementation (`add_to_graph` method).
- [`Component`](./ontology_populator/implementations/core/component.py): Base class for components. Contains all the
  information related to the component (name, implementation, exposed parameters, and overriden parameters), and is
  responsible for creating the RDF triples for the component (`add_to_graph` method).
- [`Parameter`](./ontology_populator/implementations/core/parameter.py): Base class for parameters. Contains all the
  information related to the parameter.
- [`Transformation`](./ontology_populator/implementations/core/transformation.py): Base class for transformations.
  Contains all the information related to the transformation (query and language). It also contains the two specialized
  transformations: `CopyTransformation` and `LoaderTransformation`.

The subclasses of the base classes defined the `knime` package are all defined in
the [`knime_implementation.py` file](./ontology_populator/implementations/knime/knime_implementation.py), and add the
KNIME-specific information necessary to translate the RDF triples into KNIME workflows.

The [`knime` package](./ontology_populator/implementations/knime) also contains several implemented implementations and
components, which can be checked for reference.

The [`knime_miner.py`](./ontology_populator/implementations/knime/knime_miner.py) script can be used to generate a
skeleton of the implementations available in KNIME. It takes as input a JSON file containing the information of the
KNIME Nodes ([`nodeDocumentation.json`](./ontology_populator/sources/nodeDocumentation.json)), and creates a hierarchy
of Python packages. This can be used as a starting point to define the implementations in the ontology.

However, the public documentation doesn't provide many of the necessary information, specially for the parameters, so to
define the implementations and components, the following steps are recommended:

1. Open KNIME and create a workflow with the desired nodes.
2. Save the workflow as a KNIME workflow (`.knwf` file).
3. Decompress the KNIME workflow (the `.knwf` is a zip file).
4. Check the config file of the nodes you want to define (there is a directory for each node with a `settings.xml` file
   inside).
5. Define the parameters. There has to be a Parameter for every leaf tag in the `model` tag.
6. Define the components. There has to be at least one Component for every Implementation, specifying which parameters
   are exposed and which are overridden.

[//]: # (## Non-API interaction)

[//]: # ()
[//]: # (Besides using the API, it is also possible to execute the individual components of the system.)

[//]: # ()
[//]: # (### Dataset Annotator)

[//]: # ()
[//]: # (Utility script to annotate csv datasets with ontology terms. Reads all the csv files in the [`datasets`]&#40;./dataset_annotator/datasets&#41; directory )

[//]: # (and outputs the annotated datasets in the [`annotated_datasets`]&#40;./dataset_annotator/annotated_datasets&#41; directory. Must be run from the )

[//]: # (`dataset_annotator` directory.)

[//]: # ()
[//]: # (```bash)

[//]: # (cd dataset_annotator)

[//]: # (python3 main.py)

[//]: # (```)

[//]: # ()
[//]: # (### Pipeline Generator)

[//]: # ()
[//]: # (The pipeline generator can be used to generate workflows using the ontology and some user input.  )

[//]: # (It has to be run from the `pipeline_generator` directory.)

[//]: # ()
[//]: # (```shell)

[//]: # (cd pipeline_generator)

[//]: # (python3 pipeline_generator.py)

[//]: # (```)

[//]: # ()
[//]: # (It will ask for the intent name &#40;which can be whatever you want&#41;, the dataset name &#40;which must be an annotated existing)

[//]: # (dataset&#41;, and the problem name &#40;which must be an existing problem. As of now, only Classification implemented&#41;. )

[//]: # (It will also ask for a folder to store the generated workflows.)

[//]: # ()
[//]: # (```)

[//]: # (Introduce the intent name [DescriptionIntent]:  )

[//]: # (Introduce the data name [titanic.csv]: )

[//]: # (Introduce the problem name [Classification]: )

[//]: # (Introduce the folder to save the workflows:)

[//]: # (```)

[//]: # ()
[//]: # (You can use the default values for the three first questions for a quick example.)

[//]: # ()
[//]: # (## Pipeline Translator)

[//]: # ()
[//]: # (The pipeline translator will translate the ontology-represented workflows into KNIME workflows.  )

[//]: # (It has to be run from the `pipeline_translator` directory.)

[//]: # ()
[//]: # (```shell)

[//]: # (cd pipeline_translator)

[//]: # (python3 pipeline_translator.py)

[//]: # (```)

[//]: # ()
[//]: # (It will ask for a source directory &#40;which must contain the ontology-represented workflows&#41; and a destination directory,)

[//]: # (where the translated workflows will be stored. It will also ask whether you want to keep the KNIME workflows in the)

[//]: # (folder format or not.  )

[//]: # (The folder format is just the `.knwf` file decompressed. If you are testing or debugging the)

[//]: # (translation, it will make it easier to check the generated workflows &#40;you can still just decompress the workflow)

[//]: # (yourself&#41;.)

[//]: # ()
[//]: # (```)

[//]: # (Source folder:)

[//]: # (Destination folder:)

[//]: # (Keep workflows in folder format? [Y/n]:)

[//]: # (```)

[//]: # ()
[//]: # (You can also use the translator in non-interactive mode, by passing the source and destination folders as parameters.)

[//]: # ()
[//]: # (```shell)

[//]: # (python workflow_translator.py <source_folder> <destination_folder>)

[//]: # (python workflow_translator.py --keep <source_folder> <destination_folder>)

[//]: # (```)
