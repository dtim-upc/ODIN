{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Any, List, Dict, Optional, Union, Set, Type\n",
    "\n",
    "from pyshacl import validate\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(os.path.join('..'))))\n",
    "from common import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_intent_iri(intent_graph: Graph) -> URIRef:\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX tb: <{tb}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a tb:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_task(intent_graph: Graph, intent_iri: URIRef) -> Tuple[URIRef, URIRef]:\n",
    "    dataset_task_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT ?dataset ?task\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a tb:Intent .\n",
    "        {intent_iri.n3()} tb:overData ?dataset .\n",
    "        ?task tb:tackles {intent_iri.n3()} .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_task_query).bindings[0]\n",
    "    return result['dataset'], result['task']\n",
    "\n",
    "def get_intent_params(intent_graph: Graph, intent_iri: URIRef) -> List[Dict[str, Any]]:\n",
    "    params_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT ?param ?value\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a tb:Intent .\n",
    "        {intent_iri.n3()} tb:usingParameter ?param_value .\n",
    "        ?param_value tb:forParameter ?param .\n",
    "        ?param_value tb:has_value ?value .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(params_query).bindings\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph: Graph, intent_iri: Optional[URIRef] = None) -> \\\n",
    "        Tuple[URIRef, URIRef, List[Dict[str, Any]], URIRef]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, task = get_intent_dataset_task(intent_graph, intent_iri)\n",
    "    # params = get_intent_params(intent_graph, intent_iri)\n",
    "\n",
    "    return dataset, task, intent_iri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_implementation_input_specs(ontology: Graph, implementation: URIRef) -> List[List[URIRef]]:\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} tb:specifiesInput ?spec .\n",
    "            ?spec a tb:DataSpec ;\n",
    "                tb:hasTag ?shape ;\n",
    "                tb:has_position ?position .\n",
    "            ?shape a tb:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology: Graph, implementation: URIRef) -> List[List[URIRef]]:\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} tb:specifiesOutput ?spec .\n",
    "            ?spec a tb:DataSpec ;\n",
    "                tb:hasTag ?shape ;\n",
    "                tb:has_position ?position .\n",
    "            ?shape a tb:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph: Graph, shape: URIRef) -> List[URIRef]:\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology: Graph, task_iri: URIRef) -> \\\n",
    "        List[Tuple[URIRef, List[URIRef]]]:\n",
    "    # if intent_parameters is None:\n",
    "    #     intent_parameters = []\n",
    "    # intent_params_match = [f'tb:hasParameter {param.n3()} ;' for param in intent_parameters]\n",
    "    # intent_params_separator = '            \\n'\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a tb:Implementation ;\n",
    "            tb:implements ?algorithm .\n",
    "        ?algorithm a tb:Algorithm ;\n",
    "            tb:solves ?task .\n",
    "        ?task tb:subTaskOf* {task_iri.n3()} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a tb:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_component_implementation(ontology: Graph, component: URIRef) -> URIRef:\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        PREFIX cb: <{cb}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} tb:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "def get_implementation_components(ontology: Graph, implementation: URIRef) -> List[URIRef]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component tb:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]\n",
    "\n",
    "def find_components_to_satisfy_shape(ontology: Graph, shape: URIRef, only_learners: bool = True) -> List[URIRef]:\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            ?implementation a tb:{'Learner' if only_learners else ''}Implementation ;\n",
    "                tb:specifiesOutput ?spec .\n",
    "            ?spec tb:hasDatatag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components\n",
    "\n",
    "def identify_data_io(ontology: Graph, ios: List[List[URIRef]], return_index: bool = False) -> Union[int, List[URIRef]]:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology:\n",
    "                return i if return_index else io_shapes\n",
    "            \n",
    "def identify_model_io(ontology: Graph, ios: List[List[URIRef]], return_index: bool = False) -> Union[int, List[URIRef]]:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX cb: <{cb}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* cb:Model .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* cb:Model .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "\n",
    "def satisfies_shape(data_graph: Graph, shacl_graph: Graph, shape: URIRef, focus: URIRef) -> bool:\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_target_class(ontology: Graph, shape: URIRef) -> URIRef:\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']\n",
    "\n",
    "\n",
    "def get_implementation_parameters(ontology: Graph, implementation: URIRef) -> Dict[\n",
    "    URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?parameter ?value ?order ?condition\n",
    "        WHERE {{\n",
    "            <{implementation}> tb:hasParameter ?parameter .\n",
    "            ?parameter tb:hasDefaultValue ?value ;\n",
    "                       tb:has_condition ?condition ;\n",
    "                       tb:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['order'], param['condition']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_overriden_parameters(ontology: Graph, component: URIRef) -> Dict[\n",
    "    URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?parameter ?parameterValue ?position ?condition\n",
    "        WHERE {{\n",
    "            {component.n3()} tb:overridesParameter ?parameterSpecification .\n",
    "            ?parameterSpecification tb:hasValue ?parameterValue .\n",
    "            ?parameter tb:specifiedBy ?parameterSpecification ;\n",
    "                       tb:has_position ?position ;\n",
    "                       tb:has_condition ?condition .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['parameterValue'], param['position'], param['condition']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology: Graph, component: URIRef) -> Dict[URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    implementation = get_component_implementation(ontology, component)\n",
    "    implementation_params = get_implementation_parameters(ontology, implementation)\n",
    "    component_params = get_component_overriden_parameters(ontology, component)\n",
    "    implementation_params.update(component_params)\n",
    "    return implementation_params\n",
    "\n",
    "\n",
    "def perform_param_substitution(graph: Graph, parameters: Dict[URIRef, Tuple[Literal, Literal, Literal]],\n",
    "                               inputs: List[URIRef]) -> Dict[URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    keys = list(parameters.keys())\n",
    "    for param in keys:\n",
    "        value, order, condition = parameters[param]\n",
    "        if condition.value is not None and condition.value != '':\n",
    "            feature_types = get_inputs_feature_types(graph, inputs)\n",
    "            if condition.value == '$$INTEGER_COLUMN$$' and int not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "            if condition.value == '$$STRING_COLUMN$$' and str not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "            if condition.value == '$$FLOAT_COLUMN$$' and float not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "        if isinstance(value.value, str) and '$$LABEL$$' in value.value:\n",
    "            new_value = value.replace('$$LABEL$$', f'{get_inputs_label_name(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '$$NUMERIC_COLUMNS$$' in value.value:\n",
    "            new_value = value.replace('$$NUMERIC_COLUMNS$$', f'{get_inputs_numeric_columns(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '$$CSV_PATH$$' in value.value:\n",
    "            new_value = value.replace('$$CSV_PATH$$', f'{get_csv_path(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '&amp;' in value.value:\n",
    "            new_value = value.replace('&amp;', '&')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def add_step(graph: Graph, pipeline: URIRef, task_name: str, component: URIRef,\n",
    "             parameters: Dict[URIRef, Tuple[Literal, Literal, Literal]], order: int,\n",
    "             previous_task: Union[None, list, URIRef] = None, inputs: Optional[List[URIRef]] = None,\n",
    "             outputs: Optional[List[URIRef]] = None) -> URIRef:\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = ab.term(task_name)\n",
    "    graph.add((pipeline, tb.hasStep, step))\n",
    "    graph.add((step, RDF.type, tb.Step))\n",
    "    graph.add((step, tb.runs, component))\n",
    "    graph.add((step, tb.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, tb.IO))\n",
    "        graph.add((in_node, tb.hasData, input))\n",
    "        graph.add((in_node, tb.has_position, Literal(i)))\n",
    "        graph.add((step, tb.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, tb.IO))\n",
    "        graph.add((out_node, tb.hasData, output))\n",
    "        graph.add((out_node, tb.has_position, Literal(o)))\n",
    "        graph.add((step, tb.hasOutput, out_node))\n",
    "    for parameter, (value, _, _) in parameters.items():\n",
    "        param_value = BNode()\n",
    "        graph.add((step, tb.hasParameterValue, param_value))\n",
    "        graph.add((param_value, tb.forParameter, parameter))\n",
    "        graph.add((param_value, tb.has_value, value))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, tb.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, tb.followedBy, step))\n",
    "    return step\n",
    "\n",
    "\n",
    "def get_component_transformations(ontology: Graph, component: URIRef) -> List[URIRef]:\n",
    "    transformation_query = f'''\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> tb:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]\n",
    "\n",
    "\n",
    "def get_inputs_label_name(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    label_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isLabel true ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "\n",
    "        }}\n",
    "    \"\"\"\n",
    "    return graph.query(label_query).bindings[0]['label'].value\n",
    "\n",
    "\n",
    "def get_inputs_numeric_columns(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    columns_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isFeature true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "            FILTER(?type IN (dmop:Float, dmop:Int, dmop:Number, dmop:Double, dmop:Long, dmop:Short, dmop:Integer))\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(columns_query).bindings\n",
    "    return ','.join([x['label'].value for x in columns])\n",
    "\n",
    "\n",
    "def get_csv_path(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    path = next(graph.objects(data_input, dmop.path), True)\n",
    "    return path.value\n",
    "\n",
    "\n",
    "def get_inputs_feature_types(graph: Graph, inputs: List[URIRef]) -> Set[Type]:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    columns_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?type\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isFeature true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(columns_query).bindings\n",
    "    mapping = {\n",
    "        dmop.Float: float,\n",
    "        dmop.Int: int,\n",
    "        dmop.Integer: int,\n",
    "        dmop.Number: float,\n",
    "        dmop.Double: float,\n",
    "        dmop.String: str,\n",
    "    }\n",
    "    return set([mapping[x['type']] for x in columns])\n",
    "\n",
    "\n",
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True) -> None:\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]) -> None:\n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, io: List[URIRef],\n",
    "                            specs: List[List[URIRef]]) -> None:\n",
    "    assert len(io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation: URIRef, inputs: List[URIRef],\n",
    "                            outputs: List[URIRef]):\n",
    "    input_index = next(ontology.objects(transformation, tb.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, tb.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component: URIRef, inputs: List[URIRef],\n",
    "                                 outputs: List[URIRef],\n",
    "                                 parameters: Dict[URIRef, Tuple[Literal, Literal, Literal]]) -> None:\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, tb.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        elif (transformation, RDF.type, tb.LoaderTransformation) in ontology:\n",
    "            continue\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX tb: <{tb}>\n",
    "PREFIX ab: <{ab}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, tb.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for param, (value, order, _) in parameters.items():\n",
    "                query = query.replace(f'$param{order + 1}', f'{value.n3()}')\n",
    "                query = query.replace(f'$parameter{order + 1}', f'{value.n3()}')\n",
    "            workflow_graph.update(query)\n",
    "\n",
    "\n",
    "def get_step_name(workflow_name: str, task_order: int, implementation: URIRef) -> str:\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def add_loader_step(ontology: Graph, workflow_graph: Graph, workflow: URIRef, dataset_node: URIRef) -> URIRef:\n",
    "    loader_component = cb.term('component-csv_local_reader')\n",
    "    loader_step_name = get_step_name(workflow.fragment, 0, loader_component)\n",
    "    loader_parameters = get_component_parameters(ontology, loader_component)\n",
    "    loader_parameters = perform_param_substitution(workflow_graph, loader_parameters, [dataset_node])\n",
    "    return add_step(workflow_graph, workflow, loader_step_name, loader_component, loader_parameters, 0, None, None,\n",
    "                    [dataset_node])\n",
    "\n",
    "\n",
    "def build_workflow_train_test(workflow_name: str, ontology: Graph, dataset: URIRef, main_component: URIRef,\n",
    "                              split_component: URIRef, transformations: List[URIRef]) -> Tuple[Graph, URIRef]:\n",
    "    workflow_graph = get_graph_xp()\n",
    "    workflow = ab.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, tb.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = ab.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    loader_step = add_loader_step(ontology, workflow_graph, workflow, dataset_node)\n",
    "    task_order += 1\n",
    "\n",
    "    split_step_name = get_step_name(workflow_name, task_order, split_component)\n",
    "    split_outputs = [ab[f'{split_step_name}-output_train'], ab[f'{split_step_name}-output_test']]\n",
    "    split_parameters = get_component_parameters(ontology, split_component)\n",
    "    split_step = add_step(workflow_graph, workflow,\n",
    "                          split_step_name,\n",
    "                          split_component,\n",
    "                          split_parameters,\n",
    "                          task_order,\n",
    "                          loader_step,\n",
    "                          [dataset_node],\n",
    "                          split_outputs)\n",
    "    run_component_transformation(ontology, workflow_graph, split_component,\n",
    "                                 [dataset_node], split_outputs,\n",
    "                                 split_parameters)\n",
    "\n",
    "    task_order += 1\n",
    "\n",
    "    train_dataset_node = split_outputs[0]\n",
    "    test_dataset_node = split_outputs[1]\n",
    "\n",
    "    previous_train_step = split_step\n",
    "    previous_test_step = split_step\n",
    "\n",
    "    for train_component in [*transformations, main_component]:\n",
    "        test_component = next(ontology.objects(train_component, tb.hasApplier, True), train_component)\n",
    "        same = train_component == test_component\n",
    "\n",
    "        train_step_name = get_step_name(workflow_name, task_order, train_component)\n",
    "        test_step_name = get_step_name(workflow_name, task_order + 1, test_component)\n",
    "\n",
    "        train_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                           get_component_implementation(ontology, train_component))\n",
    "        train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "        train_transformation_inputs = [ab[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "        train_transformation_inputs[train_input_data_index] = train_dataset_node\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                train_input_specs)\n",
    "\n",
    "        train_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                             get_component_implementation(ontology, train_component))\n",
    "        train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "        train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "        train_transformation_outputs = [ab[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                train_output_specs)\n",
    "\n",
    "        train_parameters = get_component_parameters(ontology, train_component)\n",
    "        train_parameters = perform_param_substitution(workflow_graph, train_parameters, train_transformation_inputs)\n",
    "        train_step = add_step(workflow_graph, workflow,\n",
    "                              train_step_name,\n",
    "                              train_component, train_parameters, task_order, previous_train_step,\n",
    "                              train_transformation_inputs,\n",
    "                              train_transformation_outputs)\n",
    "\n",
    "        previous_train_step = train_step\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, train_component, train_transformation_inputs,\n",
    "                                     train_transformation_outputs, train_parameters)\n",
    "\n",
    "        if train_output_data_index is not None:\n",
    "            train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "        task_order += 1\n",
    "\n",
    "        test_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                          get_component_implementation(ontology, test_component))\n",
    "        test_input_data_index = identify_data_io(ontology, test_input_specs, return_index=True)\n",
    "        test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "        test_transformation_inputs = [ab[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "        test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "        test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                test_input_specs)\n",
    "\n",
    "        test_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                            get_component_implementation(ontology, test_component))\n",
    "        test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "        test_transformation_outputs = [ab[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                test_output_specs)\n",
    "\n",
    "        previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "        test_parameters = get_component_parameters(ontology, test_component)\n",
    "        test_parameters = perform_param_substitution(workflow_graph, test_parameters, test_transformation_inputs)\n",
    "        test_step = add_step(workflow_graph, workflow,\n",
    "                             test_step_name,\n",
    "                             test_component, test_parameters, task_order, previous_test_steps,\n",
    "                             test_transformation_inputs,\n",
    "                             test_transformation_outputs)\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, test_component, test_transformation_inputs,\n",
    "                                     test_transformation_outputs, test_parameters)\n",
    "\n",
    "        test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "        previous_test_step = test_step\n",
    "        task_order += 1\n",
    "\n",
    "    saver_component = cb.term('component-csv_local_writer')\n",
    "    saver_step_name = get_step_name(workflow_name, task_order + 1, saver_component)\n",
    "    saver_parameters = get_component_parameters(ontology, saver_component)\n",
    "    saver_parameters = perform_param_substitution(workflow_graph, saver_parameters, [test_dataset_node])\n",
    "    add_step(workflow_graph, workflow, saver_step_name, saver_component, saver_parameters, task_order,\n",
    "             previous_test_step, [test_dataset_node], [])\n",
    "\n",
    "    return workflow_graph, workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def build_workflows(ontology: Graph, intent_graph: Graph, destination_folder: str, log: bool = False) -> None:\n",
    "    dataset, task, intent_iri = get_intent_info(intent_graph)\n",
    "\n",
    "    if log:\n",
    "        tqdm.write(f'Intent: {intent_iri.fragment}')\n",
    "        tqdm.write(f'Dataset: {dataset.fragment}')\n",
    "        tqdm.write(f'Task: {task.fragment}')\n",
    "        # tqdm.write(f'Intent params: {intent_params}')\n",
    "        tqdm.write('-------------------------------------------------')\n",
    "\n",
    "    impls = get_potential_implementations(ontology, task)\n",
    "    components = [\n",
    "        (c, impl, inputs)\n",
    "        for impl, inputs in impls\n",
    "        for c in get_implementation_components(ontology, impl)\n",
    "    ]\n",
    "    if log:\n",
    "        for component, implementation, inputs in components:\n",
    "            tqdm.write(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "            for im_input in inputs:\n",
    "                tqdm.write(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "        tqdm.write('-------------------------------------------------')\n",
    "\n",
    "    workflow_order = 0\n",
    "\n",
    "    split_components = [\n",
    "        cb.term('component-random_absolute_train_test_split'),\n",
    "        cb.term('component-random_relative_train_test_split'),\n",
    "        cb.term('component-top_k_absolute_train_test_split'),\n",
    "        cb.term('component-top_k_relative_train_test_split'),\n",
    "    ]\n",
    "\n",
    "    for component, implementation, inputs in tqdm(components, desc='Components', position=1):\n",
    "        if log:\n",
    "            tqdm.write(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "        assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "        if log:\n",
    "            tqdm.write(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "        unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                              not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "\n",
    "        available_transformations = {\n",
    "            shape: find_components_to_satisfy_shape(ontology, shape, only_learners=True)\n",
    "            for shape in unsatisfied_shapes\n",
    "        }\n",
    "\n",
    "        if log:\n",
    "            tqdm.write(f'\\tUnsatisfied shapes: ')\n",
    "            for shape, transformations in available_transformations.items():\n",
    "                tqdm.write(f'\\t\\t{shape.fragment}: {[x.fragment for x in transformations]}')\n",
    "\n",
    "        transformation_combinations = list(\n",
    "            enumerate(itertools.product(split_components, *available_transformations.values())))\n",
    "        # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "        if log:\n",
    "            tqdm.write(f'\\tTotal combinations: {len(transformation_combinations)}')\n",
    "\n",
    "        for i, transformation_combination in tqdm(transformation_combinations, desc='Transformations', position=0,\n",
    "                                                  leave=False):\n",
    "            if log:\n",
    "                tqdm.write(\n",
    "                    f'\\t\\tCombination {i + 1} / {len(transformation_combinations)}: {[x.fragment for x in transformation_combination]}')\n",
    "\n",
    "            workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "            wg, w = build_workflow_train_test(workflow_name, ontology, dataset, component,\n",
    "                                              transformation_combination[0],\n",
    "                                              transformation_combination[1:])\n",
    "\n",
    "    #         wg.add((w, tb.generatedFor, intent_iri))\n",
    "    #         wg.add((intent_iri, RDF.type, tb.Intent))\n",
    "\n",
    "    #         if log:\n",
    "    #             tqdm.write(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "    #         wg.serialize(os.path.join(destination_folder, f'{workflow_name}.ttl'), format='turtle')\n",
    "    #         workflow_order += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No folder introduced, using default (./workflows/2024-05-21 13-53-58/)\n",
      "Directory does not exist, creating it\n",
      "Intent: ci\n",
      "Dataset: titanic.csv\n",
      "Task: Classification\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tData input: ['LabeledTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\tTotal combinations: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "                                                      \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 1 / 4: ['component-random_absolute_train_test_split']\n",
      "\t\tCombination 2 / 4: ['component-random_relative_train_test_split']\n",
      "\t\tCombination 3 / 4: ['component-top_k_absolute_train_test_split']\n",
      "\t\tCombination 4 / 4: ['component-top_k_relative_train_test_split']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "                                                       \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "                                                         \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "\u001b[A                                                      \n",
      "Components:  50%|     | 2/4 [00:00<00:00, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\u001b[A                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTotal combinations: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Transformations:   0%|          | 0/24 [00:00<?, ?it/s]  \n",
      "Components: 100%|| 4/4 [00:00<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "Workflows built in 0.3816397190093994 seconds\n",
      "Workflows saved in ./workflows/2024-05-21 13-53-58/\n"
     ]
    }
   ],
   "source": [
    "def interactive():\n",
    "    intent_graph = get_graph_xp()\n",
    "    intent = input('Introduce the intent name [DescriptionIntent]: ') or 'DescriptionIntent'\n",
    "    data = input('Introduce the data name [titanic.csv]: ') or 'titanic.csv'\n",
    "    task = input('Introduce the problem name [Description]: ') or 'Description'\n",
    "\n",
    "    intent_graph.add((ab.term(intent), RDF.type, tb.Intent))\n",
    "    intent_graph.add((ab.term(intent), tb.overData, ab.term(data)))\n",
    "    intent_graph.add((cb.term(task), tb.tackles, ab.term(intent)))\n",
    "\n",
    "    ontology = get_ontology_graph()\n",
    "\n",
    "    folder = input('Introduce the folder to save the workflows: ')\n",
    "    if folder == '':\n",
    "        folder = f'./workflows/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}/'\n",
    "        tqdm.write(f'No folder introduced, using default ({folder})')\n",
    "    if not os.path.exists(folder):\n",
    "        tqdm.write('Directory does not exist, creating it')\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    t = time.time()\n",
    "    build_workflows(ontology, intent_graph, folder, log=True)\n",
    "    t = time.time() - t\n",
    "\n",
    "    print(f'Workflows built in {t} seconds')\n",
    "    print(f'Workflows saved in {folder}')\n",
    "\n",
    "\n",
    "interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
